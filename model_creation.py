# -*- coding: utf-8 -*-
"""model_creation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bQPdp4l3b5tRltYbZd-pkovg3pu79MRq

Referenced from https://www.kaggle.com/code/alvations/n-gram-language-model-with-nltk/notebook
"""

from nltk.lm.preprocessing import padded_everygram_pipeline

import re
from nltk.tokenize import ToktokTokenizer
# See https://stackoverflow.com/a/25736515/610569
sent_tokenize = lambda x: re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', x)
# Use the toktok tokenizer that requires no dependencies.
toktok = ToktokTokenizer()
word_tokenize = word_tokenize = toktok.tokenize

text_str = ""
f = open("text_str.txt", "r")
all_lines = f.readlines()

for items in all_lines:
  text_str += str(items)
f.close()

text_str

# Tokenize the text.
token_list = [list(map(str.lower, word_tokenize(sent))) 
                  for sent in sent_tokenize(text_str)]

token_list

"""Change the value of n as per the requirement"""

n = 10
training_data, sentenses = padded_everygram_pipeline(n, token_list)

from nltk.lm import MLE
model = MLE(n) # Lets train a 3-grams model, previously we set n=3

model.fit(training_data, sentenses)
print(model.vocab)

"""Save the model"""

import dill as pickle 

model_name = "LMn-" + str(n) + ".pkl"

with open(model_name, 'wb') as fout:
    pickle.dump(model, fout)